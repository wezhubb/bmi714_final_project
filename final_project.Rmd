---
title: "BMI 714 Final Project"
author: "Wenzhu Ye, Lindsay Cheng"
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: true
    toc_depth: 3
    toc_float: true
  pdf_document: default
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_engines$set(txt = function(options) {
  code <- paste(options$code, collapse = "\n")
  knitr::engine_output(options, code, NULL)
})
```

```{r}
# Load Library here
library(dplyr)
library(ggplot2)
library(lubridate)
library(tidyr)
library(ggcorrplot)
```

# View Data

```{r}
nhanes <- read.csv("data/BMI714_NHANES2020_Data.csv", header = T)
nhanes_var_name <- read.csv("data/BMI714_NHANES_VariableDictionary.csv", header = T)
# Rename the columns in nhanes
colnames(nhanes) <- ifelse(colnames(nhanes) %in% nhanes_var_name$BMI_714_Variable_Name, 
                           nhanes_var_name$Variable_Name[match(colnames(nhanes),nhanes_var_name$BMI_714_Variable_Name)], 
                           colnames(nhanes))  # Keep names unchanged if not in mapping
nhanes %>% nrow()
```
<<<<<<< HEAD
```{r}
nhanes %>% select(RIDAGEYR) %>% filter(RIDAGEYR >= 18) %>% nrow()
```
=======
>>>>>>> 0db7d6069c0c2837ca47a57e11ab094e9a25b926

```{r}
nhanes_var_name %>% dplyr::select(Data_File_Description) %>% unique()
```

weight in kg: BMXWT

```{r}
# Calculate the percentage of missing values for each column
missing_percent <- colSums(is.na(nhanes)) / nrow(nhanes)

# Filter columns with less than 10% missingness
filtered_df <- nhanes[, missing_percent < 0.20]
dim(filtered_df)
```

```{r}
# Extract the column names from the filtered dataset
filtered_columns <- colnames(filtered_df)

# Filter var_dict for rows where col_name matches the filtered column names
filtered_var_dict <- nhanes_var_name[nhanes_var_name$Variable_Name %in% filtered_columns, ]
```

```{r}
nhanes %>% 
  dplyr::select(ALQ111, RIDAGEYR) %>% 
  filter(RIDAGEYR > 18) %>%
  filter(rowSums(is.na(.)) == 1) %>%  # Check if all selected columns are NA
  nrow()
```

```{r}
nhanes %>% 
  dplyr::select(ALQ111, RIDAGEYR) %>% 
  filter(RIDAGEYR > 18)
```

```{r}
nhanes %>% dplyr::select(BMXWT) %>% is.na() %>% sum()
```

## independent variable consideration

variable details in nhanes website:
<https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?Cycle=2017-2020>

Dairy product used(DBQ197:
<https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_DBQ.htm#DBQ197>):
Greater consumption of total dairy products prevention of weight gain in
middle-aged and elderly women who are initially normal weight.
<https://pmc.ncbi.nlm.nih.gov/articles/PMC4807700/>

Type of Milk(DBQ223A, DBQ223B, DBQ223C, DBQ223D, DBQ223E, DBQ223U:
<https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_DBQ.htm#DBQ223A>):
1%/skim milk is more common among overweight/obese preschoolers,
potentially reflecting the choice of parents to give overweight/obese
children low-fat milk to drink. Nevertheless, 1%/skim milk does not
appear to restrain body weight gain between 2–4 years in this age range,
emphasizing a need for weight-targeted recommendations with a greater
evidence basis. <https://pmc.ncbi.nlm.nih.gov/articles/PMC4439101/>
Missingness:

```{r}
(nhanes %>% 
  dplyr::select(DBQ223A, DBQ223B, DBQ223C, DBQ223D, DBQ223E, DBQ223U) %>% 
  filter(rowSums(is.na(.)) == ncol(.)) %>%  # Check if all selected columns are NA
  nrow()) / nrow(nhanes) # Count the number of such rows
```

Caffeine: caffeine with weight loss:
<https://pubmed.ncbi.nlm.nih.gov/30335479/> - high missingess:
15538/15560

```{r}
caffeine_rows <- nhanes_var_name[grep("caffeine", nhanes_var_name$Variable_Description, ignore.case = TRUE), ]
```

Income(INDFMMPC:
<https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_INQ.htm#INDFMMPC>):
<https://pubmed.ncbi.nlm.nih.gov/29306894/>

Sleep(SLQ300, SLQ310, SLQ320, SLQ330:
<https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_SLQ.htm#SLD012>):<https://pmc.ncbi.nlm.nih.gov/articles/PMC9031614/>

Smoking(SMD460:
<https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_SMQFAM.htm#SMD460>):
<https://pubmed.ncbi.nlm.nih.gov/7039293/>

Diabetes(DIQ010:
<https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_DIQ.htm#DIQ010>):
<https://www.baptisthealth.com/blog/family-health/does-diabetes-cause-weight-loss-or-weight-gain>

Frozen meals/ready to go meal/fast food/resturation: (DBD905, DBD910:
<https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_DBQ.htm#DBD905>):
<https://pmc.ncbi.nlm.nih.gov/articles/PMC4302389/>

race(RIDRETH3,
<https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_DEMO.htm#RIDRETH1>):
<https://pmc.ncbi.nlm.nih.gov/articles/PMC3464818/>

Alcohol consumption(ALQ121:
<https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_ALQ.htm#ALQ121>): -
note: the data only ask this question for people 18 age and up. Should
we classify people below don't consumption any alcohol? Missing:

```{r}
(nhanes %>% 
  dplyr::select(ALQ111, RIDAGEYR) %>% 
  filter(RIDAGEYR > 18) %>%
  filter(rowSums(is.na(.)) == 1) %>%
  nrow()) / (
nhanes %>% 
  dplyr::select(ALQ111, RIDAGEYR) %>% 
  filter(RIDAGEYR > 18) %>% nrow())
```

protein(DR2TPROT) , carb (DR2TCARB), fat (DR2TTFAT) intake: -
<https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_DR2TOT.htm#DR2TPROT> -
can have more specific group - 每个都是3673/14300 missing - continuous
variable - present in doc not in the dataset?

# Variable Choosen:

Dependent variable: weight(BMXWT)

Independent variable: Dairy product used(DBQ197) Type of Milk(DBQ223A,
DBQ223B, DBQ223C, DBQ223D, DBQ223E, DBQ223U) Frozen meals/ready to go
meal/fast food/resturation (DBD905, DBD910) Income(INDFMMPC)
Sleep(SLQ300, SLQ310, SLQ320, SLQ330) Smoking(SMD460) Diabetes(DIQ010)
Alcohol consumption(ALQ121)

Goal: how socioeconmic state (income), eating behaviour affect obesity
(BMI) in adult

# Prep data

```{r}
data <- nhanes %>% 
  dplyr::select(	
BMXBMI, DBQ197, DBQ223A, DBQ223B, DBQ223C, DBQ223D, DBQ223E, DBQ223U, DBD905, DBD910, INDFMMPC, SLQ300, SLQ310, SLQ320, SLQ330, SMD460, DIQ010, ALQ121, RIDAGEYR) %>% # select variables
  filter(!is.na(	
BMXBMI)) %>% # remove rows with missing weight
filter(RIDAGEYR >= 18) # remove rows with age < 16)

```

## BMI category

according to
<https://www.cdc.gov/bmi/adult-calculator/bmi-categories.html>

```{r}
data <- data %>%
  mutate(
    bmi_category = case_when(
      BMXBMI < 18.5 ~ "Underweight",
      BMXBMI >= 18.5 & BMXBMI < 25 ~ "Normal",
      BMXBMI >= 25 & BMXBMI < 30 ~ "Overweight",
      BMXBMI >= 30 ~ "Obesity",
      TRUE ~ NA_character_  # Handle missing or unexpected BMI values
    )
  )
```

## Process independent variable

Note: predictors are ordinal, encoding them to reflect their order.

```{r}
########### Type of Milk ###########
# For Type of Milk DBQ223A, DBQ223B, DBQ223C, DBQ223D, DBQ223E, DBQ223U, we will combine them into one variable. 
#  want to see combined effect of drinking mupltiple type of milk: Group participants who drink multiple types of milk into a single “Mixed Milk” category.
data <- data %>%
  rowwise() %>%
  mutate(milk_type = if_else(sum(!is.na(c(DBQ223A, DBQ223B, DBQ223C, DBQ223D, DBQ223E, DBQ223U))) > 1, 
                             "Mixed Milk", 
                             as.character(coalesce(DBQ223A, DBQ223B, DBQ223C, DBQ223D, DBQ223E, DBQ223U)))) %>%
  ungroup()

# rename milk_type
data <- data %>% mutate(milk_type = case_when(
  milk_type == "10" ~ '4', #Whole
  milk_type == "11" ~ '3', #2%
  milk_type == "12" ~ '1', # 1% or 0.5%
  milk_type == "13" ~ '0', #Skim
  milk_type == "14" ~ '2', #Soy, ~ 1.5%-2% 
  milk_type == "30" ~ '8', #Other
  milk_type == "Mixed Milk" ~ '9', #Mixed Milk
  milk_type == "77" ~ NA_character_, #Refused
  milk_type == "99" ~ NA_character_, #Unknown
  TRUE ~ NA_character_ # NA into Unknown
))


########### Milk consumption  ###########
data <- data %>%
  mutate(
    milk_consumption = case_when(
      DBQ197 == 0 ~ '0', #Never
      DBQ197 == 1 ~ '1', #less than once a week
      DBQ197 == 2 ~ '2', #once a week or more, but less than once a day
      DBQ197 == 3 ~ '3', # once a day or more
      DBQ197 == 4 ~ '9', #Varied
      DBQ197 == 7 ~ NA_character_, #Refused
      DBQ197 == 9 ~ NA_character_, #Unknow
      is.na(DBQ197) ~ NA_character_  # Handle missing values
    )
  ) 


########### Poverty  ###########
data <- data %>%
  mutate(
    poverty_category = case_when(
      INDFMMPC == 1 ~ '1', #Monthly poverty level index <= 1.30
      INDFMMPC == 2 ~ '2',# 1.30 < Monthly poverty level index <= 1.85
      INDFMMPC == 3 ~ '3', #> 1.85
      INDFMMPC == 7 ~ NA_character_, #Refused
      INDFMMPC == 9 ~ NA_character_, #"Don't know"
      is.na(INDFMMPC) ~ NA_character_  
    )
  )




########### Alcohol  ###########
data <- data %>%
  mutate(
    ALQ121 = if_else(RIDAGEYR < 18, 0, ALQ121),  # Assign 0 for age < 18
    alcohol_frequency = case_when(
      ALQ121 == 0 ~ '0', #"Never in the last year"
      ALQ121 == 1 ~ '10', #"Every day"
      ALQ121 == 2 ~ '9', #Nearly every day"
      ALQ121 == 3 ~ '8', #"3 to 4 times a week"
      ALQ121 == 4 ~ '7', #"2 times a week"
      ALQ121 == 5 ~ '6', #Once a week
      ALQ121 == 6 ~ '5', #2 to 3 times a month
      ALQ121 == 7 ~ '4', #Once a month
      ALQ121 == 8 ~ '3', #7 to 11 times in the last year
      ALQ121 == 9 ~ '2', #3 to 6 times in the last year
      ALQ121 == 10 ~ '1', #1 to 2 times in the last year
      ALQ121 == 77 ~ NA_character_, #"Refused"
      ALQ121 == 99 ~ NA_character_, #"Don't know"
      is.na(ALQ121) ~ NA_character_
    )
  )

########### Diabete ###########
data <- data %>%
  mutate(
    diabetes_status = case_when(
      DIQ010 == 1 ~ '1', #Yes
      DIQ010 == 2 ~ '0', #No
      DIQ010 == 3 ~ '0.5', #Borderline
      DIQ010 == 7 ~ NA_character_, # "Refused"
      DIQ010 == 9 ~ NA_character_, # "Don't know"
      is.na(DIQ010) ~ NA_character_  # Handle missing values
    ))



########### Second hand smoke ########### 
data <- data %>%
  mutate(
    household_smoking_status = case_when(
      SMD460 == 0 ~ '0', #No one in household is a smoker
      SMD460 == 1 ~ '1', #1 household member is a smoker
      SMD460 == 2 ~ '2', #2 or more household members are smokers
      SMD460 == 777 ~ NA_character_, #Refused
      SMD460 == 999 ~ NA_character_, #"Don't know"
      is.na(SMD460) ~ NA_character_  # Handle missing values
    )
  )



########### Fast food ########### 
# add the range of values together, from 0 - 90, if added value over 90, or either one is 6666, mark it as 90+, if either one is 7777 or 9999 and the other is 0 - 90 or 6666, use the other value. If both are 7777 or 9999, make as 7777 or 9999 correspsoned

data <- data %>%
  rowwise() %>%
  mutate(
    fast_food_consumption = case_when(
      # If either value is 6666 (More than 90 times), mark as "99"
      DBD905 == 6666 | DBD910 == 6666 ~ "99",
      
      # If the sum of values exceeds 90, mark as "99"
      sum(c(DBD905, DBD910), na.rm = TRUE) > 90 ~ "99",
      
      # If one value is 7777 or 9999 and the other is 0-90 or 6666, use the valid value
      DBD905 %in% c(7777, 9999) & DBD910 %in% c(0:90, 6666) ~ as.character(DBD910),
      DBD910 %in% c(7777, 9999) & DBD905 %in% c(0:90, 6666) ~ as.character(DBD905),
      
      # If both are 7777 or 9999, retain the corresponding value
      DBD905 %in% c(7777, 9999) & DBD910 %in% c(7777, 9999) ~ as.character(max(DBD905, DBD910)),
      
      # Otherwise, add the values (if both are 0-90) and return the sum
      TRUE ~ as.character(sum(c(DBD905, DBD910), na.rm = TRUE))
    )
  ) %>%
  ungroup()


########### sleep time ########### 
# Convert time to decimal hours
time_to_decimal <- function(time) {
  ifelse(
    time %in% c("77777", "99999") | is.na(time),
    NA,
    as.numeric(hms(paste0(time, ":00"))) / 3600
  )
}

data <- data %>%
  mutate(
    # Convert sleep and wake times to decimal hours
    sleep_weekdays = time_to_decimal(SLQ300),
    wake_weekdays = time_to_decimal(SLQ310),
    sleep_weekends = time_to_decimal(SLQ320),
    wake_weekends = time_to_decimal(SLQ330),
    
    # Calculate sleep hours, accounting for crossing midnight
    sleep_hours_weekdays = case_when(
      !is.na(sleep_weekdays) & !is.na(wake_weekdays) ~ 
        ifelse(wake_weekdays < sleep_weekdays, wake_weekdays + 24 - sleep_weekdays, wake_weekdays - sleep_weekdays),
      TRUE ~ NA_real_
    ),
    sleep_hours_weekends = case_when(
      !is.na(sleep_weekends) & !is.na(wake_weekends) ~ 
        ifelse(wake_weekends < sleep_weekends, wake_weekends + 24 - sleep_weekends, wake_weekends - sleep_weekends),
      TRUE ~ NA_real_
    ),
    
    # Calculate weighted sleep directly
    sleep_hours = (5 * sleep_hours_weekdays + 2 * sleep_hours_weekends) / 7
  )

```

## check age distribution

```{r}
data %>% ggplot(aes(x = RIDAGEYR)) +
  geom_histogram(binwidth = 10, fill = "blue", color = "black", alpha = 0.7) +
  labs(
    title = "Histogram of Weighted Sleep Hours",
    x = "Weighted Sleep Hours",
    y = "Frequency"
  ) +
  theme_minimal()
```

## AGG data

```{r}
agg_data <- data %>%
  dplyr::select(BMXBMI, bmi_category, RIDAGEYR, milk_type, milk_consumption, poverty_category, alcohol_frequency, diabetes_status, household_smoking_status, fast_food_consumption, sleep_hours) %>%
  rename(
    bmi = BMXBMI,
    age = RIDAGEYR
  )

head(agg_data)
  
```

# Exploratory Data Analysis

```{r}
# Summary of data
summary(agg_data)

# Check data structure
str(agg_data)
```

## Missingness

NA Summary

```{r}
library(knitr)

# Create a summary table of missing values
na_summary <- data.frame(
  Variable = colnames(agg_data),
  NA_Count = colSums(is.na(agg_data)),
  NA_Percentage = round(colSums(is.na(agg_data)) / nrow(agg_data) * 100, 2)
)

# Pretty table using kable
kable(na_summary, format = "markdown", col.names = c("Variable", "NA Count", "NA Percentage (%)"))
```

```{r}
final_data <- agg_data %>%
  filter(!is.na(milk_type), !is.na(milk_consumption), !is.na(poverty_category), !is.na(alcohol_frequency), !is.na(diabetes_status), !is.na(household_smoking_status), !is.na(fast_food_consumption), !is.na(sleep_hours))
```

```{r}
# Summary of data
summary(final_data)

# Check data structure
str(final_data)
```

## factorize Ordinal Variables

predictors are ordinal, encoding them to reflect their order.

```{r}
final_data <- final_data %>%
  mutate(bmi_category = factor(bmi_category, 
      levels = c("Underweight", "Normal", "Overweight", "Obesity"), 
      ordered = TRUE)) %>%
  mutate(milk_type = factor(
      milk_type, 
      levels = c("0", "1", "2", "3", '4', '8', '9'), 
      ordered = TRUE)) %>%
  mutate(milk_consumption = factor(
      milk_consumption, 
      levels = c("0", "1", "2", "3", '9'), 
      ordered = TRUE))

final_data <- final_data %>%
  mutate(poverty_category = as.numeric(poverty_category)) %>%
  mutate(alcohol_frequency = as.numeric(alcohol_frequency)) %>%
  mutate(diabetes_status = as.numeric(diabetes_status)) %>%
  mutate(household_smoking_status = as.numeric(household_smoking_status)) %>%
  mutate(fast_food_consumption = as.numeric(fast_food_consumption))
```

## realtionship between variable of interest

```{r, fig.width=10, fig.height=10}
custom_labels <- c(
  "Underweight" = "Under\nWeight",
  "Normal" = "Normal",
  "Overweight" = "Over\nWeight",
  "Obesity" = "Obesity"
)

facet_labels <- c(
  "poverty_category" = "Poverty Category",
  "alcohol_frequency" = "Alcohol Frequency",
  "diabetes_status" = "Diabetes Status",
  "household_smoking_status" = "Household Smoking Status",
  "fast_food_consumption" = "Fast Food Consumption",
  "sleep_hours" = "Hours of Sleep",
  "milk_type" = "Milk Type",
  "milk_consumption" = "Milk Consumption"
)


plot <- final_data %>%
  mutate(
    milk_type = as.numeric(as.character(milk_type)),
    milk_consumption = as.numeric(as.character(milk_consumption))
  ) %>%
  pivot_longer(
    cols = c(poverty_category, alcohol_frequency, diabetes_status, household_smoking_status, fast_food_consumption, sleep_hours, milk_type, milk_consumption),
    names_to = "variable",
    values_to = "value"
  ) %>%
  ggplot(aes(x = bmi_category, y = value, fill = bmi_category)) +
  geom_boxplot(alpha = 0.7, outlier.color = "red")+
  scale_x_discrete(labels = custom_labels) +
  facet_wrap(~ variable, scales = "free_y", labeller = labeller(variable = facet_labels), ncol = 4) +
  labs(
    title = "Relationship Between Variables of Interest and BMI Categories",
    x = "BMI Category",
    y = "Variable of Interest Value"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
    strip.text = element_text(size = 12), # larger facet title
    axis.title = element_text(size = 12, face = "bold")
        ) 
  ggsave("relation_btw_variable.png", plot=plot, width = 10, height = 5)
```

-   no obvious outliers

## correlation

-   ordinal variable
-   non-linear realtionship -\> use spearman correlation

```{r}
cor_matrix <- final_data %>%
  dplyr::select(poverty_category, alcohol_frequency, diabetes_status, household_smoking_status, fast_food_consumption, sleep_hours, milk_type, milk_consumption) %>%
  mutate(
    milk_type = as.numeric(as.character(milk_type)),
    milk_consumption = as.numeric(as.character(milk_consumption))
  ) %>% 
  cor(method = "spearman")

# Visualize the correlation matrix

cor_plot <- ggcorrplot(cor_matrix, method = "circle", title = "Correlation Between Predictors")
ggsave("correlation_matrix.png", plot=cor_plot)
```

```{r}
predictors <- final_data %>%
  dplyr::select(poverty_category, alcohol_frequency, diabetes_status, household_smoking_status, fast_food_consumption, sleep_hours, milk_type, milk_consumption) %>%
  mutate(
    milk_type = as.numeric(as.character(milk_type)),
    milk_consumption = as.numeric(as.character(milk_consumption))
  )

results <- data.frame(
  Var1 = character(),  # First variable
  Var2 = character(),  # Second variable
  Correlation = numeric(),
  T_Statistic = numeric(),
  P_Value = numeric(),
  stringsAsFactors = FALSE
)

for (i in 1:(ncol(predictors) - 1)) {
  for (j in (i + 1):ncol(predictors)) {
    # Get variable names
    var1 <- colnames(predictors)[i]
    var2 <- colnames(predictors)[j]
    
    # Perform correlation test
    test <- cor.test(predictors[[var1]], predictors[[var2]], method = "pearson")  # Use "spearman" if needed
    
    # Extract statistics
    correlation <- test$estimate
    t_statistic <- test$statistic
    p_value <- test$p.value
    
    # Store results in dataframe
    results <- rbind(results, data.frame(
      Var1 = var1,
      Var2 = var2,
      Correlation = correlation,
      T_Statistic = t_statistic,
      P_Value = p_value
    ))
  }
}

```

```{r}

results %>%
  dplyr::select(Var1, Var2, T_Statistic, P_Value) %>%
  rowwise() %>%
  mutate(Output = paste("For pair (", Var1, ", ", Var2, "): T-Statistic = ", T_Statistic, ", P-Value = ", P_Value, sep = "")) %>%
  pull(Output) %>%
  writeLines()
```

```{r}
results$Bonferroni_Adjusted_P <- p.adjust(results$P_Value, method = "bonferroni")

results %>%
  dplyr::select(Var1, Var2, T_Statistic, Bonferroni_Adjusted_P) %>%
  rowwise() %>%
  mutate(Output = paste("For pair (", Var1, ", ", Var2, "): T-Statistic = ", T_Statistic, ", Padj-Value = ", Bonferroni_Adjusted_P, sep = "")) %>%
  pull(Output) %>%
  writeLines()
```

## scale
```{r}
# Scaling Continuous Variables
final_data <- final_data %>%
  mutate_at(vars(sleep_hours), 
            scale)
# Confirm Scaling
summary(final_data)
```


## Transformation
```{r}
# # log transformation
# final_data <- final_data %>%
#   mutate(sleep_hours = log(sleep_hours + 1e-6))
```

# Model Building

```{r}
# Load Necessary Libraries
if (!require("glmnet")) install.packages("glmnet")
if (!require("caret")) install.packages("caret")
if (!require("randomForest")) install.packages("randomForest")
if (!require("ggplot2")) install.packages("ggplot2")

library(glmnet)
library(caret)
library(randomForest)
library(ggplot2)

# Ensure target variable is a factor
final_data$bmi_category <- as.factor(final_data$bmi_category)

# Data Preparation and Scaling
scale_columns <- c("sleep_hours", "fast_food_consumption", "poverty_category", 
                   "alcohol_frequency", "diabetes_status", "household_smoking_status")

final_data[scale_columns] <- scale(final_data[scale_columns])

# Train-Test Split
set.seed(123)
train_index <- createDataPartition(final_data$bmi_category, p = 0.8, list = FALSE)
train_data <- final_data[train_index, ]
test_data <- final_data[-train_index, ]

# Prepare Data for glmnet
x <- model.matrix(bmi_category ~ ., data = final_data)[, -1]
y <- as.factor(final_data$bmi_category)
x_train <- x[train_index, ]
y_train <- y[train_index]
x_test <- x[-train_index, ]
y_test <- y[-train_index]

# Check Split
cat("Training Rows:", nrow(train_data), "\nTesting Rows:", nrow(test_data), "\n")

# --------------------------------------------------------------------
# Hyperparameter Tuning for LASSO/Elastic Net
# --------------------------------------------------------------------
set.seed(123)
alphas <- seq(0, 1, by = 0.1)  # Alpha grid: LASSO, Ridge, Elastic Net
cv_results <- list()

# Loop over alpha values
for (a in alphas) {
  lasso_model <- cv.glmnet(x_train, y_train, family = "multinomial", alpha = a, type.measure = "class")
  cv_results[[paste0("alpha_", a)]] <- list(
    lambda_min = lasso_model$lambda.min,
    accuracy = min(lasso_model$cvm)
  )
}

# Best Alpha
best_alpha <- alphas[which.min(sapply(cv_results, function(x) x$accuracy))]
cat("Best Alpha (Elastic Net):", best_alpha, "\n")

# Train Final Model with Best Alpha
set.seed(123)
final_lasso_model <- cv.glmnet(x_train, y_train, family = "multinomial", alpha = best_alpha, type.measure = "class")
optimal_lambda <- final_lasso_model$lambda.min
cat("Optimal Lambda:", optimal_lambda, "\n")

# Predictions and Evaluation
lasso_predictions <- predict(final_lasso_model, newx = x_test, s = optimal_lambda, type = "class")
lasso_confusion <- confusionMatrix(as.factor(lasso_predictions), as.factor(y_test))
lasso_accuracy <- lasso_confusion$overall["Accuracy"]

# Plot Regularization Paths
plot(final_lasso_model$glmnet.fit, xvar = "lambda", label = TRUE)
abline(v = log(optimal_lambda), col = "red", lty = 2)  # Mark optimal lambda
legend("topright", legend = "Optimal Lambda", col = "red", lty = 2, cex = 0.8)
title("Coefficient Path (LASSO/Elastic Net)")

# Plot Cross-Validation Curve
plot(final_lasso_model, main = "Cross-Validation Curve for LASSO/Elastic Net")
abline(v = log(optimal_lambda), col = "red", lty = 2)  # Mark optimal lambda
legend("topright", legend = "Optimal Lambda", col = "red", lty = 2, cex = 0.8)

# Print Results
print("Confusion Matrix (LASSO/Elastic Net):")
print(lasso_confusion)

# Plot LASSO Confusion Matrix
lasso_conf_matrix <- table(Predicted = lasso_predictions, Actual = y_test)
lasso_conf_matrix_plot <- as.data.frame(as.table(lasso_conf_matrix))

ggplot(lasso_conf_matrix_plot, aes(x = Predicted, y = Actual, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "black", size = 5) +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Confusion Matrix Heatmap (LASSO)", x = "Predicted Category", y = "Actual Category") +
  theme_minimal()
```

```{r}
# --------------------------------------------------------------------
# Hyperparameter Tuning for Random Forest
# --------------------------------------------------------------------
set.seed(123)

# Create a tuning grid for mtry
rf_grid <- expand.grid(mtry = seq(2, ncol(train_data) - 1, by = 1))  # Adjust mtry range

# Train Random Forest with Grid Search
set.seed(123)
rf_tuned <- train(
  bmi_category ~ poverty_category + alcohol_frequency + diabetes_status +
    household_smoking_status + fast_food_consumption + sleep_hours +
    milk_type + milk_consumption,
  data = train_data,
  method = "rf",
  tuneGrid = rf_grid,
  trControl = trainControl(method = "cv", number = 5),
  importance = TRUE
)

# View the best mtry value and plot the results
print(rf_tuned)
plot(rf_tuned)

# Predictions and Evaluation
rf_tuned_predictions <- predict(rf_tuned, test_data)
rf_tuned_confusion <- confusionMatrix(rf_tuned_predictions, test_data$bmi_category)
rf_tuned_accuracy <- rf_tuned_confusion$overall["Accuracy"]

# Print Results
print("Confusion Matrix (Tuned Random Forest):")
print(rf_tuned_confusion)

# Plot Random Forest Confusion Matrix
rf_conf_matrix <- table(Predicted = rf_tuned_predictions, Actual = test_data$bmi_category)
rf_conf_matrix_plot <- as.data.frame(as.table(rf_conf_matrix))

ggplot(rf_conf_matrix_plot, aes(x = Predicted, y = Actual, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "black", size = 5) +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Confusion Matrix Heatmap (Random Forest)", x = "Predicted Category", y = "Actual Category") +
  theme_minimal()
```

```{r}
# Load required library
if (!require("gbm")) install.packages("gbm")
library(gbm)
library(caret)
library(ggplot2)

# Set up train control for cross-validation
set.seed(123)
gbm_control <- trainControl(method = "cv", number = 5)

# Hyperparameter grid for GBM
gbm_grid <- expand.grid(
  interaction.depth = c(1, 3, 5),  # Tree depth
  n.trees = seq(50, 200, 50),      # Number of boosting iterations
  shrinkage = c(0.01, 0.1),        # Learning rate
  n.minobsinnode = 10              # Minimum number of observations in terminal nodes
)

# Train Gradient Boosting Model
set.seed(123)
gbm_model <- train(
  bmi_category ~ poverty_category + alcohol_frequency + diabetes_status +
    household_smoking_status + fast_food_consumption + sleep_hours +
    milk_type + milk_consumption,
  data = train_data,
  method = "gbm",
  trControl = gbm_control,
  tuneGrid = gbm_grid,
  verbose = FALSE
)

# View the best model
print(gbm_model)
plot(gbm_model)

# Predictions and Evaluation
gbm_predictions <- predict(gbm_model, test_data)
gbm_confusion <- confusionMatrix(gbm_predictions, test_data$bmi_category)
gbm_accuracy <- gbm_confusion$overall["Accuracy"]

# Print Results
print("Confusion Matrix (Gradient Boosting):")
print(gbm_confusion)

# Plot Confusion Matrix Heatmap
gbm_conf_matrix <- table(Predicted = gbm_predictions, Actual = test_data$bmi_category)
gbm_conf_matrix_plot <- as.data.frame(as.table(gbm_conf_matrix))

ggplot(gbm_conf_matrix_plot, aes(x = Predicted, y = Actual, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "black", size = 5) +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Confusion Matrix Heatmap (Gradient Boosting)", x = "Predicted Category", y = "Actual Category") +
  theme_minimal()
```

```{r}
# --------------------------------------------------------------------
# Model Comparison
# --------------------------------------------------------------------
comparison <- data.frame(
  Model = c("LASSO/Elastic Net Logistic Regression", "Tuned Random Forest", "Gradient Boosting Machine"),
  Accuracy = c(round(lasso_accuracy, 3), round(rf_tuned_accuracy, 3), round(gbm_accuracy, 3))
)

print("Model Performance Comparison:")
print(comparison)
```
# Follow up: Accuracy of Prediction in Held-Out Data
```{r}
# Predict on held-out test data
lasso_predictions <- predict(final_lasso_model, newx = x_test, s = optimal_lambda, type = "class")

# Confusion Matrix
lasso_confusion <- confusionMatrix(as.factor(lasso_predictions), as.factor(y_test))

# Print accuracy and confusion matrix
cat("Accuracy of LASSO on Test Data:", lasso_confusion$overall["Accuracy"], "\n")
cat("Confusion Matrix:")
print(lasso_confusion)

# Visualize confusion matrix as heatmap
lasso_conf_matrix <- table(Predicted = lasso_predictions, Actual = y_test)
lasso_conf_matrix_plot <- as.data.frame(as.table(lasso_conf_matrix))

ggplot(lasso_conf_matrix_plot, aes(x = Predicted, y = Actual, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "black", size = 5) +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Confusion Matrix Heatmap (LASSO)", x = "Predicted Category", y = "Actual Category") +
  theme_minimal()
```



# permutation

```{r}
# Number of permutations
n_permutations <- 1000

# Store permuted accuracies
permuted_accuracies <- numeric(n_permutations)

# Original Model Accuracy
original_accuracy <- lasso_accuracy  # Use the accuracy from your original model

# Permutation Loop
set.seed(123)
for (i in 1:n_permutations) {
  # Permute the response variable
  y_train_permuted <- sample(y_train)
  
  # Fit the LASSO model on permuted data
  permuted_model <- cv.glmnet(x_train, y_train_permuted, family = "multinomial", 
                              alpha = best_alpha, type.measure = "class")
  
  # Predict on test set using optimal lambda
  permuted_predictions <- predict(permuted_model, newx = x_test, 
                                  s = permuted_model$lambda.min, type = "class")
  
  # Calculate accuracy on test set
  permuted_confusion <- confusionMatrix(as.factor(permuted_predictions), as.factor(y_test))
  permuted_accuracies[i] <- permuted_confusion$overall["Accuracy"]
  if (i %% 100 == 0) {
    cat("Permutation", i, "\n")
  }
}
```

```{r}
# Calculate Permutation-Based p-value
p_value <- mean(permuted_accuracies >= original_accuracy)

# Print Results
cat("Original Model Accuracy:", original_accuracy, "\n")
cat("Permutation-based p-value:", p_value, "\n")

# Plot Permutation Distribution
permuted_data <- data.frame(Accuracy = permuted_accuracies)

# Plot using ggplot
plot <- ggplot(permuted_data, aes(x = Accuracy)) +
  geom_histogram(bins = 40, fill = "lightblue", color = "black") +
  labs(
    title = "Permutation Test for LASSO Model",
    x = "Accuracy",
    y = "Frequency"
  ) +
  theme_minimal()+
  theme(
    legend.position = "none",
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 10, hjust = 1),
    axis.title = element_text(size = 12, face = "bold")
        ) 
 ggsave("permutation_test.png", plot=plot, width = 10, height = 5)

```

The LASSO model achieved an accuracy of 0.998 on the test set, which is extremely high.

Permutation-Based p-Value is 0 means that none of the permuted models (with randomly shuffled response variable) achieved an accuracy equal to or greater than the original model’s accuracy which indicates that the observed accuracy of the LASSO model is highly unlikely to occur by random chance.

Distribution of Permuted Accuracies, The histogram shows the distribution of accuracies obtained from 1000 permutations. The permuted accuracies are clustered much lower (around ~0.43-0.44), while the original accuracy (0.998) is far outside this range, which reinforces the idea that the predictors have a strong, non-random association with the response variable (BMI category).

**Conclusion**
The LASSO model’s accuracy is statistically significant. The permutation test confirms that the high predictive performance is not due to random chance, as none of the permuted models achieved comparable accuracy. This validates the model and the importance of the selected predictors in classifying BMI categories.
